---
title: "Introduction to 'bayesMeanScale'"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Introduction to 'bayesMeanScale'}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo     = T,
  eval     = T,
  include  = T,
  comment = "#>"
)
```

This vignette provides an overview of the **bayesMeanScale** package, which is designed to compute model predictions, marginal effects, and comparisons of marginal effects for several different fixed-effect generalized linear models fit using the **rstanarm** package. In particular, these statistics are computed on the *mean* scale rather than the *link* scale for easier interpretation.

Predictions are estimated by holding one or more explanatory variables fixed at particular values and either averaging over the rows of the data (average marginal predictions, or AMP) or holding all other covariates at their means (marginal predictions at the mean, or MPM). Marginal effects can also be calculated by averaging over the data (average marginal effect, or AME) or holding covariates at their means (marginal effect at the mean, or MEM). Currently, the effects are specified only in terms of *discrete* changes. For a continuous variable, this might mean looking at the difference between the mean and the mean plus 1 standard deviation. In statistical applications, this sort of strategy is often very useful for summarizing a model. The third workhorse function of the package compares marginal effects against each other. This is particularly useful for testing non-linear interaction effects, such as one in a logistic regression.

## Predictions

### Average marginal predictions

For average marginal predictions, the goal is to get predictions at important settings of one or more of the model explanatory variables. These predictions are then averaged over the rows of the data. 

```{r, results='hide', message=F}

lapply(c('bayesMeanScale', 'dplyr', 'rstanarm', 'bayestestR'), function(x) library(x, character.only=T))

```

```{r}

# Simulate the data #

data(wells)

modelData <- wells %>%
  mutate(assoc = if_else(assoc==1, 'Y', 'N'))

summary(modelData)

m1 <- stan_glm(switch ~ dist*educ + arsenic + I(arsenic^2) + assoc, data=modelData, family=binomial, refresh=0)

```

```{r}

bayesPredsF(m1, 
            at = list(arsenic = c(.82, 1.3, 2.2), assoc=c("Y", "N")))


```

The output contains the unique values for the "at" variables, the posterior means and medians, and the lower and upper bounds of the credible interval. 

### Marginal predictions at the mean

For marginal predictions at the mean, the goal is essentially the same except that we want to hold the covariates at their means. In the example below, all explanatory variables except for "arsenic" are held at their means for the computation. Since "assoc" is a discrete variable, we hold it at the proportion of cases that equaled "Y".

```{r}

bayesPredsF(m1, 
            at       = list(arsenic = c(.82, 1.3, 2.2)), 
            at_means = T)

```

The results are slightly different than the average marginal predictions. From a computational standpoint, setting "at_means" to "TRUE" makes for a substantially faster computation. For relatively small models, this speed advantage is likely trivial, but it can make a noticeable difference when working with big data models.

## Marginal effects

### Average marginal effects

The concept of average marginal effects builds off the concept of average marginal predictions. We simply take two posterior distributions of average marginal predictions and subtract one from the other. In the example below, we see that the average expected probability of switching wells for families that had an arsenic level of 2.2 is roughly 27 percentage points greater than for a family that had an arsenic level of .82. Also, notice that the output contains the column "pd", which is the probability of direction. 

```{r}

m1AME <- bayesMargEffF(m1,
                       marginal_effect = 'arsenic',
                       start_value     = 2.2,
                       end_value       = .82)

m1AME$diffTable
head(m1AME$diffDraws)

```

We can also compute multiple marginal effects. When doing so, it is necessary to specify the start and end values in a list.

```{r}

m1AMEMultiple <- bayesMargEffF(m1,
                          marginal_effect = c('arsenic', 'dist'),
                          start_value     = list(2.2, 64.041),
                          end_value       = list(.82, 21.117))

m1AMEMultiple$diffTable

```

The "at" argument allows the user to specify particular values for one or more covariates, and they must be specified in a list. The example below specifies at values for "educ" given that we have an interaction between "dist" and "educ" in the model. If there is an interaction effect on the mean (probability) scale, we would expect the marginal effect of "dist" to be different at various levels of "educ." The final section will cover how to test these marginal effects against each other.

```{r}

m1AMEInteraction <- bayesMargEffF(m1,
                                  marginal_effect = 'dist',
                                  start_value     = 64.041,
                                  end_value       = 21.117,
                                  at              = list(educ=c(0, 5, 8)))

m1AMEInteraction$diffTable

```

### Marginal effects at the mean

Marginal effects at the mean compute the differences while holding the covariates at their means. Like the marginal predictions at the mean, specifying "at_means=T" allows for a much faster computation than specifying "at_means=F". 

```{r}

m1MEMInteraction <- bayesMargEffF(m1,
                                  marginal_effect = 'dist',
                                  start_value     = 64.041,
                                  end_value       = 21.117,
                                  at              = list(educ=c(0, 5, 8)),
                                  at_means        = T)

m1MEMInteraction$diffTable

```


## Comparing marginal effects

After computing multiple marginal effects for a model, you might like to compare them against one another. The "bayesMargCompareF" function calculates tests for all the unique pairs of marginal effects that you computed with "bayesMargEffF". In the example below, we are able to investigate the interaction effect between "dist" and "educ". We see that the marginal effect of "dist" is meaningfully different at different levels of "educ".

```{r}

bayesMargCompareF(m1AMEInteraction)

```


## References

Mize, Trenton D. 2019. "Best Practices for Estimating, Interpreting, and Presenting Non-linear Interaction Effects." *Sociological Science* 6: 81-117. 
